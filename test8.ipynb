{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"preprocess8.py\", line 11, in <module>\r\n",
      "    import jieba\r\n",
      "ImportError: No module named 'jieba'\r\n"
     ]
    }
   ],
   "source": [
    "!python preprocess8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting jieba\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c6/cb/18eeb235f833b726522d7ebed54f2278ce28ba9438e3135ab0278d9792a2/jieba-0.42.1.tar.gz (19.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 19.2MB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: jieba\n",
      "  Running setup.py bdist_wheel for jieba ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/xuehp/.cache/pip/wheels/98/4c/d7/713c0d3792b60467da4cf203f7313da907713c8eb6768b5ee8\n",
      "Successfully built jieba\n",
      "Installing collected packages: jieba\n",
      "Successfully installed jieba-0.42.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install jieba -i https://pypi.tuna.tsinghua.edu.cn/simple   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building source vocabulary...\n",
      "./data1/train.src  max tokens:  0\n",
      "./data1/train.src  max lengths:  0\n",
      "Created dictionary of size 3021 (pruned from 3021)\n",
      "Building target vocabulary...\n",
      "./data1/train.tgt  max tokens:  0\n",
      "./data1/train.tgt  max lengths:  0\n",
      "Created dictionary of size 2125 (pruned from 2125)\n",
      "Preparing training ...\n",
      "Processing ./data1/train.src & ./data1/train.tgt ...\n",
      "Prepared 1025 sentences (0 ignored due to length == 0 or > 0)\n",
      "Preparing validation ...\n",
      "Processing ./data1/valid.src & ./data1/valid.tgt ...\n",
      "Prepared 505 sentences (0 ignored due to length == 0 or > 0)\n",
      "Saving source vocabulary to './data8/.src.dict'...\n",
      "Saving target vocabulary to './data8/.tgt.dict'...\n",
      "Saving data to './data8/.train.pt'...\n"
     ]
    }
   ],
   "source": [
    "!python preprocess8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/cuda/__init__.py:95: UserWarning: \n",
      "    Found GPU0 Tesla V100-SXM2-16GB which requires CUDA_VERSION >= 9000 for\n",
      "     optimal performance and fast startup time, but your PyTorch was compiled\n",
      "     with CUDA_VERSION 8000. Please install the correct PyTorch binary\n",
      "     using instructions from http://pytorch.org\n",
      "    \n",
      "  warnings.warn(incorrect_binary_warn % (d, name, 9000, CUDA_VERSION))\n",
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.098\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting torch==0.4.1\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/4e/1bcb4688b7506c340ca6ba5b9f57f4ad3b59a193bba365bf2b51e9e4bb3e/torch-0.4.1-cp35-cp35m-manylinux1_x86_64.whl (519.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 519.5MB 76kB/s  eta 0:00:01    20% |██████▋                         | 107.2MB 11.0MB/s eta 0:00:38    32% |██████████▎                     | 166.8MB 63.9MB/s eta 0:00:06    38% |████████████▎                   | 199.0MB 65.4MB/s eta 0:00:05\n",
      "\u001b[?25hInstalling collected packages: torch\n",
      "  Found existing installation: torch 0.3.1\n",
      "    Uninstalling torch-0.3.1:\n",
      "      Successfully uninstalled torch-0.3.1\n",
      "Successfully installed torch-0.4.1\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 20.2.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==0.4.1 -i https://pypi.tuna.tsinghua.edu.cn/simple   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.088\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "start_decay_at:\t4\n",
      "max_grad_norm:\t5\n",
      "att_act:\ttanh\n",
      "emb_size:\t512\n",
      "hidden_size:\t512\n",
      "log:\tdata8/\n",
      "max_tgt_len:\t50\n",
      "bidirectional:\tTrue\n",
      "learning_rate_decay:\t0.1\n",
      "data:\tdata8/.train.pt\n",
      "optim:\tadam\n",
      "max_generator_batches:\t64\n",
      "shared_vocab:\tTrue\n",
      "metric:\t['rouge']\n",
      "src_vocab:\t3021\n",
      "enc_num_layers:\t2\n",
      "epoch:\t10\n",
      "dropout:\t0.0\n",
      "param_init:\t0.1\n",
      "eval_interval:\t10000\n",
      "tgt_vocab:\t2125\n",
      "beam_size:\t5\n",
      "batch_size:\t64\n",
      "learning_rate:\t0.001\n",
      "save_interval:\t3000\n",
      "num_layers:\t1\n",
      "\n",
      "seq2seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=2125, bias=True)\n",
      "    (attention): luong_attention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (softmax): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "total number of parameters: 16027725\n",
      "\n",
      "score function is \n",
      "\n",
      "/home/xuehp/git/superAE/models/attention.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = self.softmax(weights)   # batch * time\n",
      "Traceback (most recent call last):\n",
      "  File \"train8.py\", line 343, in <module>\n",
      "    main()\n",
      "  File \"train8.py\", line 332, in main\n",
      "    train(i)\n",
      "  File \"train8.py\", line 192, in train\n",
      "    loss, num_total, num_correct = model.train_model(src, src_len, tgt, tgt_len, opt.loss, updates, optim, num_oovs=num_oovs)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 85, in train_model\n",
      "    loss, num_total, num_correct = self.compute_loss(outputs, targets, loss_fn, updates)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 61, in compute_loss\n",
      "    return models.cross_entropy_loss(hidden_outputs, self.decoder, targets, self.criterion, self.config)\n",
      "  File \"/home/xuehp/git/superAE/models/loss.py\", line 190, in cross_entropy_loss\n",
      "    num_correct = pred.data.eq(targets.data).masked_select(targets.ne(dict.PAD).data).sum()\n",
      "RuntimeError: The size of tensor a (1856) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.088\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "epoch:\t10\n",
      "emb_size:\t512\n",
      "tgt_vocab:\t2125\n",
      "log:\tdata8/\n",
      "max_grad_norm:\t5\n",
      "dropout:\t0.0\n",
      "enc_num_layers:\t2\n",
      "max_tgt_len:\t50\n",
      "param_init:\t0.1\n",
      "optim:\tadam\n",
      "beam_size:\t5\n",
      "hidden_size:\t512\n",
      "data:\tdata8/.train.pt\n",
      "eval_interval:\t10000\n",
      "src_vocab:\t3021\n",
      "att_act:\ttanh\n",
      "shared_vocab:\tTrue\n",
      "learning_rate_decay:\t0.1\n",
      "start_decay_at:\t4\n",
      "max_generator_batches:\t64\n",
      "save_interval:\t3000\n",
      "metric:\t['rouge', 'bleu']\n",
      "bidirectional:\tTrue\n",
      "num_layers:\t1\n",
      "learning_rate:\t0.001\n",
      "batch_size:\t64\n",
      "\n",
      "seq2seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=2125, bias=True)\n",
      "    (attention): luong_attention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (softmax): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "total number of parameters: 16027725\n",
      "\n",
      "score function is \n",
      "\n",
      "/home/xuehp/git/superAE/models/attention.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = self.softmax(weights)   # batch * time\n",
      "Traceback (most recent call last):\n",
      "  File \"train8.py\", line 343, in <module>\n",
      "    main()\n",
      "  File \"train8.py\", line 332, in main\n",
      "    train(i)\n",
      "  File \"train8.py\", line 192, in train\n",
      "    loss, num_total, num_correct = model.train_model(src, src_len, tgt, tgt_len, opt.loss, updates, optim, num_oovs=num_oovs)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 85, in train_model\n",
      "    loss, num_total, num_correct = self.compute_loss(outputs, targets, loss_fn, updates)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 61, in compute_loss\n",
      "    return models.cross_entropy_loss(hidden_outputs, self.decoder, targets, self.criterion, self.config)\n",
      "  File \"/home/xuehp/git/superAE/models/loss.py\", line 190, in cross_entropy_loss\n",
      "    num_correct = pred.data.eq(targets.data).masked_select(targets.ne(dict.PAD).data).sum()\n",
      "RuntimeError: The size of tensor a (1856) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.089\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "beam_size:\t5\n",
      "start_decay_at:\t4\n",
      "learning_rate_decay:\t0.1\n",
      "att_act:\ttanh\n",
      "epoch:\t10\n",
      "bidirectional:\tTrue\n",
      "metric:\t['rouge', 'bleu']\n",
      "param_init:\t0.1\n",
      "shared_vocab:\tTrue\n",
      "num_layers:\t1\n",
      "learning_rate:\t0.001\n",
      "enc_num_layers:\t2\n",
      "tgt_vocab:\t2125\n",
      "max_tgt_len:\t50\n",
      "eval_interval:\t10000\n",
      "log:\tdata8/\n",
      "batch_size:\t64\n",
      "data:\tdata8/.train.pt\n",
      "optim:\tadam\n",
      "src_vocab:\t3021\n",
      "max_generator_batches:\t64\n",
      "max_grad_norm:\t5\n",
      "hidden_size:\t512\n",
      "dropout:\t0.0\n",
      "emb_size:\t512\n",
      "save_interval:\t3000\n",
      "\n",
      "seq2seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=2125, bias=True)\n",
      "    (attention): luong_attention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (softmax): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "total number of parameters: 16027725\n",
      "\n",
      "score function is \n",
      "\n",
      "/home/xuehp/git/superAE/models/attention.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = self.softmax(weights)   # batch * time\n",
      "Traceback (most recent call last):\n",
      "  File \"train8.py\", line 347, in <module>\n",
      "    main()\n",
      "  File \"train8.py\", line 333, in main\n",
      "    train(i)\n",
      "  File \"train8.py\", line 193, in train\n",
      "    loss, num_total, num_correct = model.train_model(src, src_len, tgt, tgt_len, opt.loss, updates, optim, num_oovs=num_oovs)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 85, in train_model\n",
      "    loss, num_total, num_correct = self.compute_loss(outputs, targets, loss_fn, updates)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 61, in compute_loss\n",
      "    return models.cross_entropy_loss(hidden_outputs, self.decoder, targets, self.criterion, self.config)\n",
      "  File \"/home/xuehp/git/superAE/models/loss.py\", line 190, in cross_entropy_loss\n",
      "    num_correct = pred.data.eq(targets.data).masked_select(targets.ne(dict.PAD).data).sum()\n",
      "RuntimeError: The size of tensor a (1856) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "# 修改代码\n",
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building source vocabulary...\n",
      "./data1/train.src  max tokens:  0\n",
      "./data1/train.src  max lengths:  0\n",
      "Created dictionary of size 3021 (pruned from 3021)\n",
      "Building target vocabulary...\n",
      "./data1/train.tgt  max tokens:  0\n",
      "./data1/train.tgt  max lengths:  0\n",
      "Created dictionary of size 2125 (pruned from 2125)\n",
      "Preparing training ...\n",
      "Processing ./data1/train.src & ./data1/train.tgt ...\n",
      "Prepared 1025 sentences (0 ignored due to length == 0 or > 0)\n",
      "Preparing validation ...\n",
      "Processing ./data1/valid.src & ./data1/valid.tgt ...\n",
      "Prepared 505 sentences (0 ignored due to length == 0 or > 0)\n",
      "Saving source vocabulary to './data8/.src.dict'...\n",
      "Saving target vocabulary to './data8/.tgt.dict'...\n",
      "Saving data to './data8/.train.pt'...\n"
     ]
    }
   ],
   "source": [
    "!python preprocess8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.087\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "num_layers:\t1\n",
      "beam_size:\t5\n",
      "metric:\t['rouge', 'bleu']\n",
      "max_generator_batches:\t64\n",
      "tgt_vocab:\t2125\n",
      "batch_size:\t64\n",
      "start_decay_at:\t4\n",
      "epoch:\t10\n",
      "enc_num_layers:\t2\n",
      "emb_size:\t512\n",
      "dropout:\t0.0\n",
      "bidirectional:\tTrue\n",
      "learning_rate:\t0.001\n",
      "save_interval:\t3000\n",
      "shared_vocab:\tTrue\n",
      "att_act:\ttanh\n",
      "learning_rate_decay:\t0.1\n",
      "max_grad_norm:\t5\n",
      "hidden_size:\t512\n",
      "log:\tdata8/\n",
      "eval_interval:\t10000\n",
      "data:\tdata8/.train.pt\n",
      "max_tgt_len:\t50\n",
      "src_vocab:\t3021\n",
      "optim:\tadam\n",
      "param_init:\t0.1\n",
      "\n",
      "seq2seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=2125, bias=True)\n",
      "    (attention): luong_attention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (softmax): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "total number of parameters: 16027725\n",
      "\n",
      "score function is \n",
      "\n",
      "/home/xuehp/git/superAE/models/attention.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = self.softmax(weights)   # batch * time\n",
      "Traceback (most recent call last):\n",
      "  File \"train8.py\", line 347, in <module>\n",
      "    main()\n",
      "  File \"train8.py\", line 333, in main\n",
      "    train(i)\n",
      "  File \"train8.py\", line 193, in train\n",
      "    loss, num_total, num_correct = model.train_model(src, src_len, tgt, tgt_len, opt.loss, updates, optim, num_oovs=num_oovs)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 85, in train_model\n",
      "    loss, num_total, num_correct = self.compute_loss(outputs, targets, loss_fn, updates)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 61, in compute_loss\n",
      "    return models.cross_entropy_loss(hidden_outputs, self.decoder, targets, self.criterion, self.config)\n",
      "  File \"/home/xuehp/git/superAE/models/loss.py\", line 190, in cross_entropy_loss\n",
      "    num_correct = pred.data.eq(targets.data).masked_select(targets.ne(dict.PAD).data).sum()\n",
      "RuntimeError: The size of tensor a (1856) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1596678858134  1596679116199  1596679343964\r\n",
      "1596679076982  1596679284245  lcsts.yaml\r\n"
     ]
    }
   ],
   "source": [
    "!ls data8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4616\r\n",
      "drwxrwxr-x  8 xuehp xuehp    4096 Aug  6 10:02 .\r\n",
      "drwxrwxr-x 10 xuehp xuehp    4096 Aug  6 10:02 ..\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:56 1596678858134\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:57 1596679076982\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:58 1596679116199\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 10:01 1596679284245\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 10:02 1596679343964\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:33 .ipynb_checkpoints\r\n",
      "-rw-rw-r--  1 xuehp xuehp     427 Aug  6 09:57 lcsts.yaml\r\n",
      "-rw-rw-r--  1 xuehp xuehp   28947 Aug  6 10:02 .src.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp   20007 Aug  6 10:02 .tgt.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp 4632884 Aug  6 10:02 .train.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -all data8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data8/.train.pt data8/train.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data8/.src.dict data8/src.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp data8/.tgt.dict data8/tgt.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 9196\r\n",
      "drwxrwxr-x  8 xuehp xuehp    4096 Aug  6 10:03 .\r\n",
      "drwxrwxr-x 10 xuehp xuehp    4096 Aug  6 10:02 ..\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:56 1596678858134\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:57 1596679076982\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:58 1596679116199\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 10:01 1596679284245\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 10:02 1596679343964\r\n",
      "drwxrwxr-x  2 xuehp xuehp    4096 Aug  6 09:33 .ipynb_checkpoints\r\n",
      "-rw-rw-r--  1 xuehp xuehp     427 Aug  6 09:57 lcsts.yaml\r\n",
      "-rw-rw-r--  1 xuehp xuehp   28947 Aug  6 10:02 .src.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp   28947 Aug  6 10:03 src.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp   20007 Aug  6 10:02 .tgt.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp   20007 Aug  6 10:03 tgt.dict\r\n",
      "-rw-rw-r--  1 xuehp xuehp 4632884 Aug  6 10:02 .train.pt\r\n",
      "-rw-rw-r--  1 xuehp xuehp 4632884 Aug  6 10:03 train.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls -all data8/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xuehp/git/superAE/data/utils.py:18: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  return AttrDict(yaml.load(open(path, 'r')))\n",
      "loading data...\n",
      "\n",
      "loading time cost: 0.088\n",
      "building model...\n",
      "\n",
      "use attention activation tanh\n",
      "/home/xuehp/anaconda3/envs/env_superAE/lib/python3.5/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "log:\tdata8/\n",
      "dropout:\t0.0\n",
      "epoch:\t10\n",
      "learning_rate:\t0.001\n",
      "src_vocab:\t3021\n",
      "learning_rate_decay:\t0.1\n",
      "param_init:\t0.1\n",
      "start_decay_at:\t4\n",
      "bidirectional:\tTrue\n",
      "hidden_size:\t512\n",
      "batch_size:\t64\n",
      "att_act:\ttanh\n",
      "enc_num_layers:\t2\n",
      "metric:\t['rouge', 'bleu']\n",
      "max_grad_norm:\t5\n",
      "data:\tdata8/.train.pt\n",
      "eval_interval:\t10000\n",
      "tgt_vocab:\t2125\n",
      "max_generator_batches:\t64\n",
      "emb_size:\t512\n",
      "optim:\tadam\n",
      "num_layers:\t1\n",
      "shared_vocab:\tTrue\n",
      "beam_size:\t5\n",
      "max_tgt_len:\t50\n",
      "save_interval:\t3000\n",
      "\n",
      "seq2seq(\n",
      "  (encoder): rnn_encoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): LSTM(512, 512, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (decoder): rnn_decoder(\n",
      "    (embedding): Embedding(3021, 512)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.0)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=2125, bias=True)\n",
      "    (attention): luong_attention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (softmax): Softmax()\n",
      "      (tanh): Tanh()\n",
      "    )\n",
      "    (dropout): Dropout(p=0.0)\n",
      "  )\n",
      "  (criterion): CrossEntropyLoss()\n",
      "  (log_softmax): LogSoftmax()\n",
      ")\n",
      "\n",
      "total number of parameters: 16027725\n",
      "\n",
      "score function is \n",
      "\n",
      "/home/xuehp/git/superAE/models/attention.py:32: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weights = self.softmax(weights)   # batch * time\n",
      "Traceback (most recent call last):\n",
      "  File \"train8.py\", line 347, in <module>\n",
      "    main()\n",
      "  File \"train8.py\", line 333, in main\n",
      "    train(i)\n",
      "  File \"train8.py\", line 193, in train\n",
      "    loss, num_total, num_correct = model.train_model(src, src_len, tgt, tgt_len, opt.loss, updates, optim, num_oovs=num_oovs)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 85, in train_model\n",
      "    loss, num_total, num_correct = self.compute_loss(outputs, targets, loss_fn, updates)\n",
      "  File \"/home/xuehp/git/superAE/models/seq2seq.py\", line 61, in compute_loss\n",
      "    return models.cross_entropy_loss(hidden_outputs, self.decoder, targets, self.criterion, self.config)\n",
      "  File \"/home/xuehp/git/superAE/models/loss.py\", line 190, in cross_entropy_loss\n",
      "    num_correct = pred.data.eq(targets.data).masked_select(targets.ne(dict.PAD).data).sum()\n",
      "RuntimeError: The size of tensor a (1856) must match the size of tensor b (64) at non-singleton dimension 1\n"
     ]
    }
   ],
   "source": [
    "!python train8.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
